{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load the Dataset\nimport os\n\nimage_dataset_path = '../input/flickr8k-imageswithcaptions/Flickr8k_Dataset/Flicker8k_Dataset'\ncaption_dataset_path = '../input/flickr8k-imageswithcaptions/Flickr8k_text/Flickr8k.token.txt'","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, we will load the caption file and store it in a captions dictionary for later purposes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the caption file & read it\ndef load_caption_file(path):\n    \n    # dictionary to store captions\n    captions_dict = {}\n    \n    # iterate through the file\n    for caption in open(path):\n    \n        # caption has format-> 1000268201_693b08cb0e.jpg#0  A child in a pink dress is climbing up a set of stairs in an entry way .\n        tokens = caption.split()\n        caption_id, caption_text = tokens[0].split('.')[0], tokens[1:]\n        caption_text = ' '.join(caption_text)\n        \n        # save it in the captions dictionary\n        if caption_id not in captions_dict:\n            captions_dict[caption_id] = caption_text\n        \n    return captions_dict\n\n# call the function\ncaptions_dict = load_caption_file(caption_dataset_path)\n","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, load the image file. Extract the feature of each image using **VGG16 (Visual Geometry Group)** CNN model feature extractor and map it to the image id in image dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the important\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.layers import Input","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input() is used to instantiate a Keras tensor. Pass the dimensions of the image into shape parameter. \n# VGG16 model uses image shape (224, 224, 3) where 3 is the color RGB. \ninput_layer = Input(shape=(224, 224, 3))\n\n# load the VGG16 model. include_top is set to False because we just want the feature extractor part not the dense layers. \nmodel = VGG16(include_top=False, input_tensor = input_layer)\n\nprint(model.summary())    ","execution_count":4,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58892288/58889256 [==============================] - 0s 0us/step\nModel: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The following piece of code took 3 hrs on CPU power to run."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"images_dict = {}\n# loop through the images\nfor image_path in os.listdir(image_dataset_path):\n    \n    image = load_img(image_dataset_path + '/' + image_path)\n    # convert the image pixels into array\n    image = img_to_array(image)\n    # reshape the image for the vgg16 model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # prepare the images for vgg model\n    image = preprocess_input(image)\n    # extract features from the image\n    feature = model.predict(image, verbose=0)\n    # extract the image id from the image_name\n    image_id = image_path.split('.')[0]\n    # store images and its features into \n    images_dict[image_id] = feature\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# save the image features into pickle file\nfrom pickle import dump\n\ndump(images_dict, open('image_features_dictionary.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I am saving this pickle file for the later use and you can download it from my datasets"},{"metadata":{},"cell_type":"markdown","source":"Preprocess the captions dataset \n\n1) Convert the captions into lowercase\n\n2) Tokenize the captions into different tokens\n\n3) Remove all the punctuations from the tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean the captions\nimport string\n\n# dictionary to store the cleaned captions\nnew_captions_dict = {}\n\n# prepare translation table for removing punctuation. third argument is the list of punctuations we want to remove\ntable = str.maketrans('', '', string.punctuation)\n\n# loop through the dictionary\nfor caption_id, caption_text in captions_dict.items():\n    # tokenize the caption_text\n    caption_text = caption_text.split()\n    # convert it into lower case\n    caption_text = [token.lower() for token in caption_text]\n    # remove punctuation from each token\n    caption_text = [token.translate(table) for token in caption_text]\n    # remove all the single letter tokens like 'a', 's'\n    caption_text = [token for token in caption_text if len(token)>1]\n    # store the cleaned captions\n    new_captions_dict[caption_id] = ' '.join(caption_text)\n    ","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare the word embeddings to feed into the embedding layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\n# initialise tokenizer\ntokenizer = Tokenizer()\n# create word count dictionary on the captions list\ntokenizer.fit_on_texts(list(new_captions_dict.values()))\nvocab_len = len(tokenizer.word_index) + 1\nsequences = tokenizer.texts_to_sequences(list(new_captions_dict.values()))\n\n# pad the sequences to be of fixed size\nmax_len = max(len(seq) for seq in sequences)\npadded_sequences = pad_sequences(sequences, maxlen= max_len, padding='post')\n\n# one hot encode all the captions\none_hot_encoded = to_categorical(padded_sequences, num_classes= vocab_len)\n# ensure the shape \none_hot_encoded = one_hot_encoded.reshape(len(new_captions_dict), max_len, vocab_len)\n\nprint(\"captions size \", len(new_captions_dict))\nprint(\"vocabulary size \", vocab_len)\nprint(\"sequences size \", max_len)\nprint(\"shape of the word embeddings \", one_hot_encoded.shape)","execution_count":6,"outputs":[{"output_type":"stream","text":"captions size  8092\nvocabulary size  4485\nsequences size  28\nshape of the word embeddings  (8092, 28, 4485)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# More to come... Stay Tuned!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}